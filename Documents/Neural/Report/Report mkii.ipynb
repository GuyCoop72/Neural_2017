{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Computation Assignment\n",
    "### Brendan Case - bkc721 - 1801421\n",
    "### Guy Coop - gtc434 - 1447634\n",
    "### Vasileios Mizaridis - vxm716 - 1844216\n",
    "### Priyanka Mohata - pxm374 - 1341274\n",
    "### Liangye Yu - lxy736 - 1810736"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "Image Recognition has, in recent history been relatively ”solved” in the sense that algorithms have now been recorded outperforming humans in simple image recogntion tasks. However regional image recognition where the algorithm is tasked with recognizing multiple images inside a whole chaotic scene is still an emerging field. Our team analysed the most promising methods of regional image recognition, and implemented our own solution to a simplified regional recognition task. Our solution was able to locate and classify objects with some degree of accuracy, however was not able to submit an accepted result to the kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "For this project, our team was assigned the task of implementing a regional image recognition system.\n",
    "\n",
    "## Data Sets\n",
    "The data set provided was given in the following format:\n",
    "\n",
    "- Data: 400x400 RGB image files in .jpg format\n",
    "\n",
    "- Labels: each image has a corresponding text file that describes the location of each of the predefined objects in the image. This location was given as pairs of integers describing horizontal runs of pixels that form a rectangular bounding box around the object. If the object was not present in the image it was given as [object 1 0] meaning that it had a run length of 0 pixels.\n",
    "\n",
    "### Training Set: \n",
    "The training set provided contains 14,625 (image, label) pairs that includes image files and label files that describe the objects and bouding boxes. for conducting experiments, this data set should be subdivided into a Training set and a Validation set to measure how parameter changes affect the accuracy of the results.\n",
    "\n",
    "### Test Set:\n",
    "The test set contained 2500 images that are given to simulate unseen data coming into the system. These images do not have the associated label files, and our task is to produce label files that describe the location and class of objects in each of the test image files.\n",
    "\n",
    "## Our Aims\n",
    "Our aims for this project are as follows:\n",
    "\n",
    "- Discover and Analyse currently exisitng method of performing regional image recognition\n",
    "\n",
    "- Produce our own implementation of one of these methods, using any necessary packages or source code segments as necessary.\n",
    "\n",
    "- Conduct experiments to optimize the recognition system in terms of bounding box locations, and object classification.\n",
    "\n",
    "- Produce a set of conclusions about the effectiveness of the various recognition methods, and the optimal parameters of our implementation for the data set provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design\n",
    "For this task our Initial instict was to attempt to solve it with a simple feedforward neural network. And whilst this would have been adequate for a simple classification task, we determined it would not provide a good solution to this region bounded classification task. From there we exaimned other possible methods of solving the task. The first sub-method we investigated was using Selective Search to identify regions of interest inside the image. We then expanded this to determine other pre-established algorithms that make use of selective search and how they compare to other similar algorithms.\n",
    "\n",
    "## Selective Search\n",
    "Selective Search is an algorithm used for regional image searching. It works by performing image processing to segment an image by multiple factors such as: Colour, Texture, an Brightness, in order to try and seperate multiple object inside an image. once these sections have been seperated. rectangular bounding boxes can be formed around the objects so that they can be passed to an image recognizer network.\n",
    "\n",
    "## Regional Convolutional Neural Networks (RCNNs)\n",
    "Given the regional nature of this task, The first option that should be analysed is ”Regional Convultional Neural Networks” and their successors. There are three implementations of this algorithm that will be examined:\n",
    "\n",
    "- R-CNN [GDDM14]\n",
    "\n",
    "- Fast R-CNN [Gir15]\n",
    "\n",
    "- Faster R-CNN [RHGS15]\n",
    "\n",
    "### R-CNN \n",
    "R-CNN [GDDM14] makes use of selective search to generate the region proposals. it typically produces around 2000 region proposals per image, these regions are then sent forward to the CNN in order to determine if they contain an object in the dataset, and what that object is. Once the objects have been detected in the bounding boxes, regression algorithms are used to tighten the bounding boxes more accurately around the objects.\n",
    "\n",
    "### Fast R-CNN\n",
    "Fast R-CNN [Gir15] is an update on the original R-CNN technique that was developed in 2015, it acheives approximately a 9x speed-up on the original at train time, and over 200x speed-up at test time. It does this by unifying the training phase of the boudinding boxes, and the object classification algorithm into a single round of training, rather than having to train the two algorithms seperately.\n",
    "It also makes use of Region-of-Interest (RoI) pooling layers. \"RoI max pooling works by reducing the $h*w$ RoI window into an H*W grid of sub-windows of approximate size h/H * w/W and then max-pooling the values in each sub-window into the corresponding output grid cell.\" [Gir15]\n",
    "\n",
    "### Faster R-CNN\n",
    "Faster R-CNN [RHGS15] is another significat update on the Fast R-CNN technique that acheives another dramatic speedup. This algorithm was designed as part of an attempt at real time regional image recognition, and as such is able to operate in almost real time.\n",
    "Similar to Fast R-CNN it makes use of the RoI pooling layer to create a significant imporvement in performance over traditional R-CNN. Faster R-CNN also introduces a Region Proposal Network (RPN). The RPN shares convolutional features with the detection network. This allows it to provide almost \"cost-free\" region proposals to the system.\n",
    "Fast R-CNN is recorded to be able to operate at approximately 5 frames-per-second (fps) when running on a GPU, meaning that it could be used for real time applications.\n",
    "\n",
    "## You Only Look Once (YOLO)\n",
    "Another option we explored is a technique called “You Only Look Once” (YOLO) [RF16]. YOLO was named as such because the algorithm centers around only performing a single pass across the image, rather than having to analyse the same data multiple times. This techniques, introduced in 2015, was unlike any other object detection model currently in-use. Unlike R-CNN techniques which pass the images through multiple networks, YOLO only requires one network evaluation. \n",
    "\n",
    "In 2016, YOLO9000(also called YOLOv2) was introduced which had most of the same functionality offered in the initial version of YOLO, however it was modified to improve the localization and recall issue present in YOLO while trying to maintain the classification accuracy and speed. In [RF16] the YOLOv2 technique is recorded to be able to operate close to 45 frames-per-second (fps) on a normal CPU. This is faster than the original YOLO and also it understands more generalized object representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first attempt resembled the original R-CNN algorithm outlined in [GDDM14]. This was chosen from the initial intuition that by training a CNN with 3 convolutional layers and 2 fully connected layers, we could achieve good accuracy for the classification component of the problem on simple image sets such as MNIST. This CNN was implemented using the layers module in TensorFlow [Goo17]. We felt perhaps decent accuracy could be obtained by using this trained CNN on multiple proposed regions, then giving these regions a 'score' based on how confident the classification was, and taking the best scoring region among largely intersecting regions as the output region. This is essentially the R-CNN algorithm. We decided to use the same region proposal algorithm, SelectionSearch, as the original authors, but with different parameters to reduce region proposals and favor larger bounding boxes. Finding which parameters favored these preferences was one source of experimentation throughout the project. In addition, we found simply removing region proposals with certain extreme aspect ratios was a reasonable assumption for this data set.\n",
    "\n",
    "These regions, along with the regions provided in the training text files, were converted to 120 * 120 arrays of rgb tuples using Skimage (SciKit Image processing library [sidt17]) and numpy reshaping tools, which could then be used to train the CNN.\n",
    "\n",
    "In the interest of time and sanity, this implementation was largely discarded in favor of the more ready-to-use implementation of YOLO from Darknet. In order to comply with the requirement to implement code in Python, we made use of a Python wrapper provided by Darknet. To use this, we wrote a script which loaded a trained network with provided weights and made predictions for each image in the test set, appending the predictions in the proper format to a submission.txt file, used to submit our predictions. We also adjusted one of the provided networks, 'tiny-yolo,' and trained our own weights using the training image and labels provided; however, given the limited time these weights never reached the point of having an average loss below 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class: \"data_handling\"\n",
    "includes: \n",
    "- a settable image_size variable that can be changed for different data sets.\n",
    "- a full list of the names of each object that is part of the data set.  \n",
    "- a method to get a dictionary of corner locations of bouding boxes based on the label files given in the dataset.\n",
    "- a pair of methods to convert the full set of label files to the style used by YOLO.\n",
    "- a method to convert the output of our systems to the label format required by the data set.\n",
    "- a method to load the data set into the program to be manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import skimage.io as skio\n",
    "import skimage.transform as transform\n",
    "\n",
    "class data_handler:\n",
    "# A class to manage data type changes from one format to another:\n",
    "# Manages input text labels into format used by predesigned networks\n",
    "# Also manages outputs produced and converts them to the output format.\n",
    "\n",
    "    image_size = [400, 400]\n",
    "\n",
    "    object_list = [\n",
    "        \"aeroplane\",\n",
    "        \"bicycle\",\n",
    "        \"bird\",\n",
    "        \"boat\",\n",
    "        \"bottle\",\n",
    "        \"bus\",\n",
    "        \"car\",\n",
    "        \"cat\",\n",
    "        \"chair\",\n",
    "        \"cow\",\n",
    "        \"diningtable\",\n",
    "        \"dog\",\n",
    "        \"horse\",\n",
    "        \"motorbike\",\n",
    "        \"person\",\n",
    "        \"pottedplant\",\n",
    "        \"sheep\",\n",
    "        \"sofa\",\n",
    "        \"train\",\n",
    "        \"tvmonitor\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_bounding_boxes(label):\n",
    "        # from an input pixel list file, generate a dictionary of bounding boxes\n",
    "        # the dictionary contains a list of TL, BR bounding boxes under each bounding_boxes[ind]\n",
    "        # i.e. bounding boxes['aeroplane'] will return a list of all the bounding boxes for planes in a single image\n",
    "        def pixel_list_to_bboxes(pixel_list):\n",
    "            pixel_loc_corners = []\n",
    "            bboxes = []\n",
    "            #start pixel location, current left pixel location, run length\n",
    "            if pixel_list[0][0] == 1 and pixel_list[0][1] == 0:\n",
    "                return bounding_boxes\n",
    "            pixel_loc_corners.append([pixel_list[0][0], pixel_list[0][0], pixel_list[0][1]])\n",
    "            for pair in pixel_list[1:]:\n",
    "                break_var = 0\n",
    "                for ind, objs_found in enumerate(pixel_loc_corners):\n",
    "                    if objs_found[1] + data_handler.image_size[1] == pair[0]:\n",
    "                        # not a new object\n",
    "                        pixel_loc_corners[ind][1] = pair[0]\n",
    "                        break_var = 1\n",
    "                        break\n",
    "                if break_var == 0:\n",
    "                    pixel_loc_corners.append([pair[0], pair[0], pair[1]])\n",
    "\n",
    "            # finished building pixel_loc_corners\n",
    "            for corner in pixel_loc_corners:\n",
    "                x1 = ((corner[0]) % data_handler.image_size[0]) + 1\n",
    "                y1 = int(corner[0] / data_handler.image_size[1])\n",
    "                x2 = ((corner[1] + corner[2] - 1) % data_handler.image_size[0]) + 1\n",
    "                y2 = int((corner[1] + corner[2]) / data_handler.image_size[1])\n",
    "                bboxes.append([[x1, y1],[x2, y2]])\n",
    "\n",
    "            return bboxes\n",
    "\n",
    "\n",
    "        pixel_list_dict = {}\n",
    "        for ind, line in enumerate(label.readlines()):\n",
    "            pixel_list_dict[data_handler.object_list[ind]] = map(int, (line.split(',')[1].split()))\n",
    "            pixel_list_dict[data_handler.object_list[ind]] = np.reshape(\n",
    "                pixel_list_dict[data_handler.object_list[ind]],\n",
    "                [len(pixel_list_dict[data_handler.object_list[ind]])/2, 2])\n",
    "\n",
    "        bounding_boxes = {}\n",
    "        for obj in data_handler.object_list:\n",
    "            bounding_boxes[obj] = pixel_list_to_bboxes(pixel_list_dict[obj])\n",
    "\n",
    "        return bounding_boxes\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_yolo_text_files(input_file_location, output_file_location):\n",
    "        # Reformats a single text data file into the format required to train for the YOLO network\n",
    "        output_string = \"\"\n",
    "        f = open(input_file_location)\n",
    "        bboxes = data_handler.get_bounding_boxes(f)\n",
    "        for ind, obj in enumerate(data_handler.object_list):\n",
    "            if bboxes[obj] != bboxes:\n",
    "                for bbox in bboxes[obj]:\n",
    "                    x = 0.00125 * (float(bbox[0][0]) + float(bbox[1][0]))\n",
    "                    y = 0.00125 * (float(bbox[0][1]) + float(bbox[1][1]))\n",
    "                    width = (float(bbox[1][0]) - float(bbox[0][0])) / 400.\n",
    "                    height = (float(bbox[1][1]) - float(bbox[0][1])) / 400.\n",
    "                    output_string += (str(ind) + \" \" + str(x) + \" \" + str(y) + \" \" + str(width) + \" \" + str(height) + '\\n')\n",
    "\n",
    "        outfile = open(output_file_location, 'w')\n",
    "        outfile.write(output_string)\n",
    "        outfile.close()\n",
    "        return output_string\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_create_yolo_labels(input_dir, output_dir):\n",
    "        # converts a full set of labels into the yolo required format\n",
    "        txt_files = [file for file in os.listdir(input_dir) if file.endswith(\".txt\")]\n",
    "        for file in txt_files:\n",
    "            data_handler.get_yolo_text_files(input_dir + file, output_dir + file)\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_output_file(bounding_boxes, filename):\n",
    "        #generate an output pixel list from the bounding boxes dictionary\n",
    "        # inputs: a list of bounding boxes for a single image\n",
    "        #         the file name of the image, i.e. ('2007_00042.jpg')\n",
    "        #\n",
    "        # output: a string that contains the entire output information for the file\n",
    "        \n",
    "        def get_one_pixel_list(bounding_box):\n",
    "            # generates a list of pixels for a rectangular bounding box\n",
    "            pixel_list = []\n",
    "            h_run = bounding_box[1][0] - bounding_box[0][0]\n",
    "            for j in range(bounding_box[0][1], bounding_box[1][1]):\n",
    "                # perform vertical steps down\n",
    "                pixel_list.append([bounding_box[0][0] + (data_handler.image_size[1] * j), h_run])\n",
    "            return pixel_list\n",
    "\n",
    "        def sort_pixel_list(pixel_list):\n",
    "            # sorts pixel list and manages possible overlaps\n",
    "            pixel_list = sorted(pixel_list, key=lambda x:x[0])\n",
    "\n",
    "            #check if overlap is possible, if so perform compressions stage\n",
    "            return pixel_list\n",
    "\n",
    "        def str_pixel_list(pixel_list):\n",
    "            # convert a pixel list into a space seperated string\n",
    "            pix_string = \"\"\n",
    "            for pair in pixel_list:\n",
    "                pix_string += str(pair[0]) + \" \" + str(pair[1]) + \" \"\n",
    "            return pix_string\n",
    "\n",
    "        def get_full_pixel_list(bounding_boxes):\n",
    "            if bounding_boxes == []:\n",
    "                return [[0, 1]]\n",
    "\n",
    "            else:\n",
    "                pixel_list = []\n",
    "                for bounding_box in bounding_boxes:\n",
    "                    pixel_list += get_one_pixel_list(bounding_box)\n",
    "\n",
    "                pixel_list = sort_pixel_list(pixel_list)\n",
    "                return pixel_list\n",
    "\n",
    "        output_string = \"\"\n",
    "        for obj in data_handler.object_list:\n",
    "            output_string = output_string + filename + \"_\" + obj + \",\"\n",
    "            pixel_list = get_full_pixel_list(bounding_boxes[obj])\n",
    "            output_string += str_pixel_list(pixel_list)\n",
    "            output_string += '\\n'\n",
    "\n",
    "        return output_string\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_training_data():\n",
    "        labels = []\n",
    "        data = []\n",
    "        for f in os.listdir(data_handler.file_path.format('train'))[:100]:\n",
    "            if f.count('.txt'):\n",
    "                bboxes = data_handler.get_bounding_boxes(open(data_handler.file_path.format('train') + '/' + f))\n",
    "                img = data_handler.file_path.format('train') + '/' + f.replace('.txt', '') + '.jpg'\n",
    "                label, datum = data_handler.build_training_array_single(bboxes, skio.imread(img))\n",
    "                labels.extend(label)\n",
    "                data.extend(datum)\n",
    "        return data, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a python wrapper for the YOLO image recognition system:\n",
    "taken from the darknet source code ../python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "../libdarknet.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3f4888eb93db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#lib = CDLL(\"/home/pjreddie/documents/darknet/libdarknet.so\", RTLD_GLOBAL)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../libdarknet.so\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRTLD_GLOBAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_width\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_width\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/ctypes/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ../libdarknet.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from ctypes import *\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "def sample(probs):\n",
    "    s = sum(probs)\n",
    "    probs = [a/s for a in probs]\n",
    "    r = random.uniform(0, 1)\n",
    "    for i in range(len(probs)):\n",
    "        r = r - probs[i]\n",
    "        if r <= 0:\n",
    "            return i\n",
    "    return len(probs)-1\n",
    "\n",
    "def c_array(ctype, values):\n",
    "    arr = (ctype*len(values))()\n",
    "    arr[:] = values\n",
    "    return arr\n",
    "\n",
    "class BOX(Structure):\n",
    "    _fields_ = [(\"x\", c_float),\n",
    "                (\"y\", c_float),\n",
    "                (\"w\", c_float),\n",
    "                (\"h\", c_float)]\n",
    "\n",
    "class IMAGE(Structure):\n",
    "    _fields_ = [(\"w\", c_int),\n",
    "                (\"h\", c_int),\n",
    "                (\"c\", c_int),\n",
    "                (\"data\", POINTER(c_float))]\n",
    "\n",
    "class METADATA(Structure):\n",
    "    _fields_ = [(\"classes\", c_int),\n",
    "                (\"names\", POINTER(c_char_p))]\n",
    "\n",
    "    \n",
    "\n",
    "#lib = CDLL(\"/home/pjreddie/documents/darknet/libdarknet.so\", RTLD_GLOBAL)\n",
    "lib = CDLL(\"../libdarknet.so\", RTLD_GLOBAL)\n",
    "lib.network_width.argtypes = [c_void_p]\n",
    "lib.network_width.restype = c_int\n",
    "lib.network_height.argtypes = [c_void_p]\n",
    "lib.network_height.restype = c_int\n",
    "\n",
    "predict = lib.network_predict\n",
    "predict.argtypes = [c_void_p, POINTER(c_float)]\n",
    "predict.restype = POINTER(c_float)\n",
    "\n",
    "set_gpu = lib.cuda_set_device\n",
    "set_gpu.argtypes = [c_int]\n",
    "\n",
    "make_image = lib.make_image\n",
    "make_image.argtypes = [c_int, c_int, c_int]\n",
    "make_image.restype = IMAGE\n",
    "\n",
    "make_boxes = lib.make_boxes\n",
    "make_boxes.argtypes = [c_void_p]\n",
    "make_boxes.restype = POINTER(BOX)\n",
    "\n",
    "free_ptrs = lib.free_ptrs\n",
    "free_ptrs.argtypes = [POINTER(c_void_p), c_int]\n",
    "\n",
    "num_boxes = lib.num_boxes\n",
    "num_boxes.argtypes = [c_void_p]\n",
    "num_boxes.restype = c_int\n",
    "\n",
    "make_probs = lib.make_probs\n",
    "make_probs.argtypes = [c_void_p]\n",
    "make_probs.restype = POINTER(POINTER(c_float))\n",
    "\n",
    "detect = lib.network_predict\n",
    "detect.argtypes = [c_void_p, IMAGE, c_float, c_float, c_float, POINTER(BOX), POINTER(POINTER(c_float))]\n",
    "\n",
    "reset_rnn = lib.reset_rnn\n",
    "reset_rnn.argtypes = [c_void_p]\n",
    "\n",
    "load_net = lib.load_network\n",
    "load_net.argtypes = [c_char_p, c_char_p, c_int]\n",
    "load_net.restype = c_void_p\n",
    "\n",
    "free_image = lib.free_image\n",
    "free_image.argtypes = [IMAGE]\n",
    "\n",
    "letterbox_image = lib.letterbox_image\n",
    "letterbox_image.argtypes = [IMAGE, c_int, c_int]\n",
    "letterbox_image.restype = IMAGE\n",
    "\n",
    "load_meta = lib.get_metadata\n",
    "lib.get_metadata.argtypes = [c_char_p]\n",
    "lib.get_metadata.restype = METADATA\n",
    "\n",
    "load_image = lib.load_image_color\n",
    "load_image.argtypes = [c_char_p, c_int, c_int]\n",
    "load_image.restype = IMAGE\n",
    "\n",
    "rgbgr_image = lib.rgbgr_image\n",
    "rgbgr_image.argtypes = [IMAGE]\n",
    "\n",
    "predict_image = lib.network_predict_image\n",
    "predict_image.argtypes = [c_void_p, IMAGE]\n",
    "predict_image.restype = POINTER(c_float)\n",
    "\n",
    "network_detect = lib.network_detect\n",
    "network_detect.argtypes = [c_void_p, IMAGE, c_float, c_float, c_float, POINTER(BOX), POINTER(POINTER(c_float))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section of code was written by our team to run the darknet system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: '../darknet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e5afd3f15531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../darknet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mtest_fpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/brendan/Documents/Neural_2017/Documents/NC_data/test/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '../darknet'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test function to try out the wrapper for simple classification.\n",
    "Calls the internal predict_image and then rates each class, returning the most likely class\n",
    "\"\"\"\n",
    "def classify(net, meta, im):\n",
    "    out = predict_image(net, im)\n",
    "    res = []\n",
    "    for i in range(meta.classes):\n",
    "        res.append((meta.names[i], out[i]))\n",
    "    res = sorted(res, key=lambda x: -x[1])\n",
    "    return res\n",
    "\n",
    "\"\"\"\n",
    "Wrapper for yolo-detection. Takes in a neural net, class info, and an image and creates bounding boxes and classification \n",
    "predictions for each region.\n",
    "Returns a list of predictions, each containing a label, a confidence rating, and the central coordinates and width, height of the region\n",
    "\"\"\"\n",
    "def detect(net, meta, image, thresh=.5, hier_thresh=.5, nms=.45):\n",
    "    im = load_image(image, 0, 0)\n",
    "    boxes = make_boxes(net)\n",
    "    probs = make_probs(net)\n",
    "    num =   num_boxes(net)\n",
    "    print num\n",
    "    network_detect(net, im, thresh, hier_thresh, nms, boxes, probs)\n",
    "    res = []\n",
    "    for j in range(num):\n",
    "        for i in range(meta.classes):\n",
    "            if probs[j][i] > 0:\n",
    "                res.append((meta.names[i], probs[j][i], (boxes[j].x, boxes[j].y, boxes[j].w, boxes[j].h)))\n",
    "    res = sorted(res, key=lambda x: -x[1])\n",
    "    free_image(im)\n",
    "    free_ptrs(cast(probs, POINTER(c_void_p)), num)\n",
    "    print res\n",
    "    print\n",
    "    return res\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    os.chdir(\"../darknet\")\n",
    "    test_fpath = '/Users/brendan/Documents/Neural_2017/Documents/NC_data/test/'\n",
    "    sub = open('submission.txt', 'w+')\n",
    "    sub.write('image_cat,pixels\\n')\n",
    "    net = load_net(\"cfg/yolo.cfg\", 'yolo.weights', 0) #use a pretrained neural net for now\n",
    "    meta = load_meta(\"cfg/coco.data\")\n",
    "    for f in os.listdir(test_fpath):\n",
    "        if f.count('.jpg'):\n",
    "            bbox_dict = {}\n",
    "            #get the detections\n",
    "            detections = detect(net, meta, test_fpath + f)\n",
    "            for d in detections:\n",
    "                #fix up the data for the data handler\n",
    "                dw = int(d[2][2]) // 2\n",
    "                dh = int(d[2][3]) // 2\n",
    "                tl = [int(d[2][0] - dw), int(d[2][1]) - dh]\n",
    "                br = [int(d[2][0]) + dw, int(d[2][1]) + dh]\n",
    "                if d[0] in bbox_dict.keys():\n",
    "                    bbox_dict[d[0]].append([tl, br])\n",
    "                else:\n",
    "                    bbox_dict[d[0]] = [[tl, br]]\n",
    "            for label in data_handler.object_list:\n",
    "                if label not in bbox_dict.keys():\n",
    "                    bbox_dict[label] = []\n",
    "            line = data_handler.generate_output_file(bbox_dict, f)\n",
    "            print line\n",
    "            sub.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "Because of the continuing struggle with implementation, there were few opportunities for rigorous experimentation during this project. Despite this, we were able to tweak with a few parameters throughout the project. When using SelectiveSearch, we were looking for parameter values which would produce a smaller number of boxes while still keeping the good ones with high probability. The algorithm took the parameters scale, sigma, and minsize, where scale determined the tendency for larger regions, sigma the bias towards grouping nearby objects of similar color together, and minsize the minimum length of the flattened pixel array. We ran some tests on random small groups of images from the test set, and found a scale of 400, a sigma of 0.8, and a minsize of 1000 to produce promising results, but a minsize of 400 largely kept the good large region proposals, while also producing many smaller regions to increase likelyhood of finding smaller objects. \n",
    "\n",
    "We also had the opportunity to experiment with training times for the neural network used in the yolo algorithm, though never test the performance resulting from this training due to time and memory constraints. As mentioned above, we chose to use the tiny-yolo network as a basis, which uses about 5 fewer convolutional layers than other yolo configurations such as standard yolo or yolo-voc. Running on CPU, tiny-yolo ran an iteration in about 5 minutes, while the extra convolutional layers took around 20 minutes. In all examples, a batch size of 64 was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "One of our first, but most important findings, was the R-CNN is unusably slow for anything to akin to real time applications when run from a CPU. We had a number of difficulties testing and implement GPU implementations, mostly due to a lack of access to machines with GPUs (The machines in the lab were fairly uncooprative: Keras, TensorFlow, and YOLO_v2 could not be made to run due to memory issues, and none of our team had their own sutiable hardware). \n",
    "\n",
    "YOLO_v2 overcomes this problem, and runs in an acceptable time on a CPU, however the output produced from the vanilla YOLO system is incompatible with the desired output format for the competition.\n",
    "\n",
    "Our code was unable to sucessfully produce a result to the competition. despite producing seemingly close to correct values. The kaggle competition rejected the submission with the error: \"Overlapping values detected: Submitted values must be unique\". Because of this our best official performance was \"NULL\", despite attempts to forcibly rectify the output labels and remove collisions. Our belief is that this error was due to the fact that our modified version of YOLO allows overlapping bounding boxes, (i.e. the pottedplant is on the diningtable and therefore the pottedplant pixels will be a subset of the diningtable pixels) but the required format of the output data does not appear to allow this.\n",
    "\n",
    "Overall, this project can be said to give a useful insight into existing methods for regional image recognition, however it made little progress towards optimizing a new system. This was in part due to the time constraints as implementing and retraining a new style of network can take over a day in some cases and there was a very limited amount of time to run experiments of this length once our team was getting close to functioning implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Collaboration\n",
    "### Brendan Case\n",
    "- Wrote the python wrapper for YOLO.\n",
    "\n",
    "- Set up training YOLO on data with Vasileios.\n",
    "\n",
    "- Investigated RCNN and tested working with region proposal systems.\n",
    "\n",
    "### Guy Coop\n",
    "- Produced data handler python class used by multiple systems to reformat the training data into the required input format for the neural network. And reformat the output from the network back into a format that matches the input.\n",
    "\n",
    "- Lead the writing of the report, and collated it into a Jupyter Notebook.\n",
    "\n",
    "### Vasileios Mizaridis\n",
    "- performed research into R-CNN using TensorFlow and Keras\n",
    "\n",
    "- Assisted writing a python Wrapper for YOLO.\n",
    "\n",
    "- Set up training and testing YOLO with our competition data.\n",
    "\n",
    "### Priyanka Mohata\n",
    "- Tried to implement multiple versions different version of YOLO using pre existing neural networks. This proved to be very difficult since there were many issues during this due to many of the codes not run correctly on a windows machine. I did try it on the Linux machine is the UG lab but were unsuccessful due to limited disk storage quota.\n",
    "\n",
    "- Ran darknet(YOLO) on a virtual linux machine.Initially ran it with its pretrained weights and tested it on a few images. After determining that YOLO was indeed a good technique, tried to modify it to fit it into our dataset and file structure.\n",
    "\n",
    "- Contributed some sections of the report\n",
    "\n",
    "### Liangye Yu\n",
    "- Found and collected information about the You Only Look Once (YOLO) System\n",
    "- Analysed and tested a pre-trained network\n",
    "- Analysed the results and attempted to retrain the network for our own data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[GDDM14] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. R-cnn for object detection. 2014.\n",
    "\n",
    "[Gir15] Ross Girshick. Fast r-cnn. arXiv:1504.08083, 2015.\n",
    "\n",
    "[Goo17] Google. Tensorflow 1.0, 2017.\n",
    "\n",
    "[RDGF15] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. arXiv:1506.02640, 2015.\n",
    "\n",
    "[RF16] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. arXiv preprint arXiv:1612.08242, 2016.\n",
    "\n",
    "[RHGS15] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv:1506.01497, 2015.\n",
    "\n",
    "[sidt17] scikit-image development team. skimage 0.13.1, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
