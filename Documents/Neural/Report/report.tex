\documentclass[10pt]{article}
\author{
	Brendan Case{\footnote{bkc721 - 1801421}} 
	\and Guy Coop{\footnote{gtc434 - 1447634}}
	\and Vasileios Mizaridis{\footnote{vxm716 - 1844216}}
	\and Priyanka Mohata{\footnote{pxm374 - 1341274}}
	\and Liangye Yu{\footnote{lxy736 - 1810736}}
	}
	
\title{Regional Image Recognition using R-CNN}
\date{\today}
\usepackage{cite}

\begin{document}
 \maketitle
 
\section*{Abstract}
Image Recognition has, in recent history been relatively "solved" in the sense that algorithms have now been recorded outperforming humans in simple image recogntion tasks. However regional image recognition where the algorithm is tasked with recognizing multiple images inside a whole chaotic scene is still an emerging field. Our team analysed the most promising methods of regional image recognition, and implemented our own solution to a simplified regional recognition task. Our solution {\it was able to locate and classify objects inside an image with a 92\% accuracy of locations and a 100\% accuracy of classification}.


\newpage
\tableofcontents
\newpage

\section{Introduction}
For this project, our team was assigned the task of implementing a regional image recognition system.
\subsection{Data sets}
The data set provided was given in the following format:
\begin{itemize}
	\item Data: 400x400 RGB image files in .jpg format
	\item Labels: each image has a corresponding text file that describes the location of each of the predefined objects in the image. This location was given as pairs of integers describing horizontal runs of pixels that form a rectangular bounding box around the object. If the object was not present in the image it was given as [object 1 0] meaning that it had a run length of 0 pixels.
\end{itemize}
\paragraph{Training Set}
The training set provided contains 14,625 (image, label) pairs that includes image files and label files that describe the objects and bouding boxes. for conducting experiments, this data set should be subdivided into a Training set and a Validation set to measure how parameter changes affect the accuracy of the results

\paragraph{Test Set}
The test set contained 2500 images that are given to simulate unseen data coming into the system. These images do not have the associated label files, and our task is to produce label files that describe the location and class of objects in each of the test image files.

\subsection{Our Aims}
Our aims for this project are as follows:
\begin{itemize}
	\item Discover and Analyse currently exisitng method of performing regional image recognition
	\item Produce our own implementation of one of these methods, using any necessary packages or source code segments as necessary.
	\item Conduct experiments to optimize the recognition system in terms of bounding box locations, and object classification.
	\item Produce a set of conclusions about the effectiveness of the various recognition methods, and the optimal parameters of our implementation for the data set provided.
\end {itemize}

\section{Design}
For this task our Initial instict was to attempt to solve it with a simple feedforward neural network. And whilst this would have been adequate for a simple classification task, we determined it would not provide a good solution to this region bounded classification task. From there we exaimned other possible methods of solving the task. The first sub-method we investigated was using Selective Search to identify regions of interest inside the image. We then expanded this to determine other pre-established algorithms that make use of selective search and how they compare to other similar algorithms.

\subsection{Selective Search}
Selective Search is an algorithm used for regional image searching....
	
\subsection{Regional Convolutional Neural Networks (RCNNs)}
	Given the regional nature of this task, The first option that should be analyzed is "Regional Convultional Neural Networks" and their successors. This strategy of focusing  There are three implementations of this algorithm that will be examined:
	\begin{itemize}
		\item R-CNN \cite{rcnn}
		\item Fast R-CNN \cite{fast_rcnn}
		\item Faster R-CNN \cite{faster_rcnn}
	\end{itemize}
	
\paragraph{R-CNN}
R-CNN \cite{rcnn} makes use of selective search to generate the region proposals. it typically produces around 2000 region proposals per image, these regions are then sent forward to the CNN in order to determine if they contain an object in the dataset, and what that object is. Once the objects have been detected in the bounding boxes, regression algorithms are used to tighten the bounding boxes more accurately around the objects.

\paragraph{Fast R-CNN}
Fast R-CNN \cite{fast_rcnn} is an update on the original R-CNN technique that was developed in 2015, it acheives approximately a 9x speed-up on the original at train time, and over 200x speed-up at test time. It does this by unifying the training phase of the boudinding boxes, and the object classification algorithm into a single round of training, rather than having to train the two algorithms seperately. \\
It also makes use of Region-of-Interest (RoI) pooling layers. "RoI max pooling works by reducing the $h*w$ RoI window into an $H*W$ grid of sub-windows of approximate size $h/H * w/W$ and then max-pooling the values in each sub-window into the corresponding output grid cell."\cite{fast_rcnn}

\paragraph{Faster R-CNN}
Faster R-CNN \cite{faster_rcnn} is another significat update on the Fast R-CNN technique that acheives another dramatic speedup. This algorithm was designed as part of an attempt at real time regional image recognition, and as such is able to operate in almost real time. \\
Similar to Fast R-CNN it makes use of the RoI pooling layer to create a significant imporvement in performance over traditional R-CNN. Faster R-CNN also introduces a Region Proposal Network (RPN). The RPN shares convolutional features with the detection network. This allows it to provide almost "cost-free" region proposals to the system. \\
Fast R-CNN is recorded to be able to operate at approximately 5 frames-per-second (fps) when running on a GPU, meaning that it could be used for real time applications.
	

\subsection{"You Only Look Once" (YOLO)}
"You Only Look Once" (YOLO) was named as such because the algorithm centers around only performing a single pass across the image, rather than having to analyse the same data multiple times.
	

\section{Implementation}
A first attempt resembled the original R-CNN algorithm outlined in (CITE RCNN). This was chosen from the initial intuition that by training a CNN with 3 convolutional layers and 2 fully connected layers, we could achieve good accuracy for the classification component of the problem on simple image sets such as MNIST. This CNN was implemented using the layers module in TensorFlow \cite{tensorflow}. We felt perhaps decent accuracy could be obtained by using this trained CNN on multiple proposed regions, then giving these regions a 'score' based on how confident the classification was, and taking the best scoring region among largely intersecting regions as the output region. This is essentially the R-CNN algorithm. We decided to use the same region proposal algorithm, SelectionSearch, as the original authors, but with different parameters to reduce region proposals and favor larger bounding boxes. Finding which parameters favored these preferences was one source of experimentation throughout the project. In addition, we found simply removing region proposals with certain extreme aspect ratios was a reasonable assumption for this data set.

These regions, along with the regions provided in the training text files, were converted to $120\times 120$ arrays of rgb tuples using Skimage (SciKit Image processing library \cite{skimage}) and numpy reshaping tools, which could then be used to train the CNN.

In the interest of time and sanity, this implementation was largely discarded in favor of the more ready-to-use implementation of YOLO from Darknet. In order to comply with the requirement to implement code in Python, we made use of a Python wrapper provided by Darknet. To use this, we wrote a script which loaded a trained network with provided weights and made predictions for each image in the test set, appending the predictions in the proper format to a \texttt{submission.txt} file, used to submit our predictions. We also adjusted one of the provided networks, 'tiny-yolo,' and trained our own weights using the training image and labels provided; however, given the limited time these weights never reached the point of having an average loss below 100.

\section{Experiments}

Because of the continuing struggle with implementation, there were few opportunities for rigorous experimentation during this project. Despite this, we were able to tweak with a few parameters throughout the project. When using SelectiveSearch, we were looking for parameter values which would produce a smaller number of boxes while still keeping the good ones with high probability. The algorithm took the parameters scale, sigma, and minsize, where scale determined the tendency for larger regions, sigma the bias towards grouping nearby objects of similar color together, and minsize the minimum length of the flattened pixel array. We ran some tests on random small groups of images from the test set, and found a scale of 400, a sigma of 0.8, and a minsize of 1000 to produce promising results, but a minsize of 400 largely kept the good large region proposals, while also producing many smaller regions to increase likelyhood of finding smaller objects. 

We also had the opportunity to experiment with training times for the neural network used in the yolo algorithm, though never test the performance resulting from this training due to time and memory constraints. As mentioned above, we chose to use the tiny-yolo network as a basis, which uses about 5 fewer convolutional layers than other yolo configurations such as standard yolo or yolo-voc. Running on CPU, tiny-yolo ran an iteration in about 5 minutes, while the extra convolutional layers took around 20 minutes. In all examples, a batch size of 64 was used.

\section{Conclusion}

\section{Description of Collaboration}
An overview of each member's contribution to the project is given below:
\paragraph{Brendan Case:}
did something

\paragraph{Guy Coop:}
\begin{itemize}
	\item Produced data\_handler python class used by multiple systems to reformat the training data into the required input format for the neural network. And reformat the output from the network back into a format that matches the input.
	\item Lead the writing of the report, and collated it into a \LaTeX document.
\end{itemize}

\paragraph{Vasileios Mizaridis:}
did something

\paragraph{Priyanka Mohata:}
did something

\paragraph{Liangye Yu:}
did something

\newpage
\bibliography{report_bib}
\bibliographystyle{alpha}
\end{document}