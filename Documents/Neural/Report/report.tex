\documentclass[10pt]{article}
\author{
	Brendan Case{\footnote{bkc721 - 1801421}} 
	\and Guy Coop{\footnote{gtc434 - 1447634}}
	\and Vasileios Mizaridis{\footnote{vxm716 - 1844216}}
	\and Priyanka Mohata{\footnote{pxm374 - 1341274}}
	\and Liangye Yu{\footnote{lxy736 - 1810736}}
	}
	
\title{Regional Image Recognition using R-CNN}
\date{\today}
\usepackage{cite}

\begin{document}
 \maketitle
 
\section*{Abstract}
Image Recognition has, in recent history been relatively "solved" in the sense that algorithms have now been recorded outperforming humans in simple image recogntion tasks. However regional image recognition where the algorithm is tasked with recognizing multiple images inside a whole chaotic scene is still an emerging field. Our team analysed the most promising methods of regional image recognition, and implemented our own solution to a simplified regional recognition task. Our solution {\it was able to locate and classify objects inside an image with a 92\% accuracy of locations and a 100\% accuracy of classification}.


\newpage
\tableofcontents
\newpage

\section{Introduction}
For this project, our team was assigned the task of implementing a regional image recognition system.
\subsection{Data sets}
The data set provided was given in the following format:
\begin{itemize}
	\item Data: 400x400 RGB image files in .jpg format
	\item Labels: each image has a corresponding text file that describes the location of each of the predefined objects in the image. This location was given as pairs of integers describing horizontal runs of pixels that form a rectangular bounding box around the object. If the object was not present in the image it was given as [object 1 0] meaning that it had a run length of 0 pixels.
\end{itemize}
\paragraph{Training Set}
The training set provided contains 14,625 (image, label) pairs that includes image files and label files that describe the objects and bouding boxes. for conducting experiments, this data set should be subdivided into a Training set and a Validation set to measure how parameter changes affect the accuracy of the results

\paragraph{Test Set}
The test set contained 2500 images that are given to simulate unseen data coming into the system. These images do not have the associated label files, and our task is to produce label files that describe the location and class of objects in each of the test image files.

\subsection{Our Aims}
Our aims for this project are as follows:
\begin{itemize}
	\item Discover and Analyse currently exisitng method of performing regional image recognition
	\item Produce our own implementation of one of these methods, using any necessary packages or source code segments as necessary.
	\item Conduct experiments to optimize the recognition system in terms of bounding box locations, and object classification.
	\item Produce a set of conclusions about the effectiveness of the various recognition methods, and the optimal parameters of our implementation for the data set provided.
\end {itemize}

\section{Design}
For this task our Initial instict was to attempt to solve it with a simple feedforward neural network. And whilst this would have been adequate for a simple classification task, we determined it would not provide a good solution to this region bounded classification task. From there we exaimned other possible methods of solving the task. The first sub-method we investigated was using Selective Search to identify regions of interest inside the image. We then expanded this to determine other pre-established algorithms that make use of selective search and how they compare to other similar algorithms.

\subsection{Selective Search}
Selective Search is an algorithm used for regional image searching....
	
\subsection{Regional Convolutional Neural Networks (RCNNs)}
	Given the regional nature of this task, The first option that should be analysed is "Regional Convultional Neural Networks" and their successors. There are three implementations of this algorithm that will be examined:
	\begin{itemize}
		\item R-CNN \cite{rcnn}
		\item Fast R-CNN \cite{fast_rcnn}
		\item Faster R-CNN \cite{faster_rcnn}
	\end{itemize}
	
\paragraph{R-CNN}
R-CNN \cite{rcnn} makes use of selective search to generate the region proposals. it typically produces around 2000 region proposals per image, these regions are then sent forward to the CNN in order to determine if they contain an object in the dataset, and what that object is. Once the objects have been detected in the bounding boxes, regression algorithms are used to tighten the bounding boxes more accurately around the objects.

\paragraph{Fast R-CNN}
Fast R-CNN \cite{fast_rcnn} is an update on the original R-CNN technique that was developed in 2015, it acheives approximately a 9x speed-up on the original at train time, and over 200x speed-up at test time. It does this by unifying the training phase of the boudinding boxes, and the object classification algorithm into a single round of training, rather than having to train the two algorithms seperately. \\
It also makes use of Region-of-Interest (RoI) pooling layers. "RoI max pooling works by reducing the $h*w$ RoI window into an $H*W$ grid of sub-windows of approximate size $h/H * w/W$ and then max-pooling the values in each sub-window into the corresponding output grid cell."\cite{fast_rcnn}

\paragraph{Faster R-CNN}
Faster R-CNN \cite{faster_rcnn} is another significat update on the Fast R-CNN technique that acheives another dramatic speedup. This algorithm was designed as part of an attempt at real time regional image recognition, and as such is able to operate in almost real time. \\
Similar to Fast R-CNN it makes use of the RoI pooling layer to create a significant imporvement in performance over traditional R-CNN. Faster R-CNN also introduces a Region Proposal Network (RPN). The RPN shares convolutional features with the detection network. This allows it to provide almost "cost-free" region proposals to the system. \\
Fast R-CNN is recorded to be able to operate at approximately 5 frames-per-second (fps) when running on a GPU, meaning that it could be used for real time applications.
	

\subsection{"You Only Look Once" (YOLO)}
"You Only Look Once" (YOLO) was named as such because the algorithm centers around only performing a single pass across the image, rather than having to analyse the same data multiple times.
	

\section{Implementation}
--Using RCNN\\
--TensorFlow Layers \cite{tensorflow}\\
--SciKit Image package\cite{skimage}\\
--YOLO\_v1\cite{yolo_v1}\\
--using YOLO\cite{yolo_v2}\\
--Darknet\\

\section{Experiments}

\section{Conclusion}

\section{Description of Collaboration}
An overview of each member's contribution to the project is given below:
\paragraph{Brendan Case:}
did something

\paragraph{Guy Coop:}
\begin{itemize}
	\item Produced data\_handler python class used by multiple systems to reformat the training data into the required input format for the neural network. And reformat the output from the network back into a format that matches the input.
	\item Lead the writing of the report, and collated it into a \LaTeX document.
\end{itemize}

\paragraph{Vasileios Mizaridis:}
did something

\paragraph{Priyanka Mohata:}
did something

\paragraph{Liangye Yu:}
did something

\newpage
\bibliography{report_bib}
\bibliographystyle{alpha}
\end{document}